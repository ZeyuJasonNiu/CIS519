{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skeleton.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeyuJasonNiu/CIS519/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0yamHN-dY0"
      },
      "source": [
        "# **CIS 419/519 Spring 2022 - Homework 2**\n",
        "\n",
        "**Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. This is the master notebook so <u>you will not be able to save your changes without copying it </u>! Once you click on that, make sure you are working on that version of the notebook so that your work is saved** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb-WLp5Z-cdy"
      },
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "np.random.seed(42)  # don't change this line\n",
        "\n",
        "import dill\n",
        "import base64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2VtEzsZ-loR"
      },
      "source": [
        "# For autogreader only, do not modify this cell. \n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjjXBdEb-p8K"
      },
      "source": [
        "# **PennGrader Setup**\n",
        "\n",
        "First, you'll need to set up the PennGrader, an autograder we are going to use throughout the semester. The PennGrader will automatically grade your answer and provide you with an instant feedback. Unless otherwise stated, you can resubmit up to a reasonable number of attempts (e.g. 100 attemptes per day). **We will only record your latest score in our backend database**. \n",
        "\n",
        "After finishing each homework assignment, you must submit your iPython notebook to gradescope before the homework deadline. Gradescope will then retrive and display your scores from our backend database. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GCTLN4G-nK2"
      },
      "source": [
        "%%capture\n",
        "!pip3 install penngrader --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLnoPRci-sTC"
      },
      "source": [
        "from penngrader.grader import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu0XYZHO-t8J"
      },
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO \n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = ...          # YOUR PENN-ID GOES HERE AS AN INTEGER#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIDTGGbo-xkf"
      },
      "source": [
        "Run the following cell to initialize the autograder. This autograder will let you submit your code directly from this notebook and immidiately get a score.\n",
        "\n",
        "**NOTE:** Remember we store your submissions and check against other student's submissions... so, not that you would, but no cheating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw_QDnZk-vvI"
      },
      "source": [
        "grader = PennGrader(homework_id = 'CIS_519_Spring22_HW2', student_id = STUDENT_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0_ydbgD0Kvf"
      },
      "source": [
        "# A helper function for grading utils\n",
        "def grader_serialize(obj):        # A helper function\n",
        "    '''Dill serializes Python object into a UTF-8 string'''\n",
        "    byte_serialized = dill.dumps(obj, recurse = True)\n",
        "    return base64.b64encode(byte_serialized).decode(\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **NOTE 1. Results of sections marked as \"manually graded\" should be submitted along with the written homework solutions.**\n",
        "\n",
        "#### **NOTE 2. If you are running into a `__builtins__' error, it's likely because you're using a function call of the form numpy.ndarray.mean(), like a.mean(). This does not play nice with PennGrader unfortunately. Please use the function call numpy.mean(a) instead.**"
      ],
      "metadata": {
        "id": "oQCyLhELJ7LX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Linear Regression**\n",
        "\n",
        "## **1.1. Linear Regression Implementation [15 pts, autograded]**\n",
        "\n",
        "In this section you will implement linear regression with both L1 and L2 regularization. Your class LinearRegression must implement the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
        "* `compute_cost(theta, X, y)`\n",
        "* `compute_gradient(theta, X, y)`\n",
        "* `fit(X, y)`\n",
        "* `has_converged(theta_old, theta_new)`\n",
        "* `predict(X)`\n",
        "\n",
        "Note that these methods have already been defined correctly for you in the LinearRegression class. **DO NOT** change the API.\n",
        "\n",
        "### **1.1.1. Cost Function [5 pts]**\n",
        "\n",
        "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
        "\n",
        "$\n",
        "\\mathcal{L}({\\theta}) = \\frac{1}{N}\\sum_{i =1}^N (h_{{\\theta}}({x}_i) - y_i)^2\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "> $h_{{\\theta}}({x}_i) = \\theta^Tx_i$\n",
        "\n",
        "Based on the regularisation penalty, you may need to add below regularisation penalty loss to MSE Loss computed previously.\n",
        "\n",
        "L1 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L_1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  |{\\theta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L_2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  {\\theta}_j^2 \n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.2. Gradient of the Cost Function [5 pts]**\n",
        "\n",
        "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.3. Convergence Check [1 pt]**\n",
        "\n",
        "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer 1.1.4 for convergence condition.\n",
        " \n",
        "---\n",
        "\n",
        "### **1.1.4. Training [3 pts]**\n",
        "\n",
        "The `fit` method should train the model via gradient descent, relying on the cost and gradient functions. The trained weights/coefficients must be stored as `theta_`. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). \n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the gradient descent algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.5. Predict [1 pt]**\n",
        "\n",
        "The `predict` function should predict the output for the data points in a given input data matrix."
      ],
      "metadata": {
        "id": "xfs6aWUhmWJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Linear Regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol : float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty : string, default = None\n",
        "        The type of regularization. The other acceptable options are l1 and l2\n",
        "    lambd : float, default = 1.0\n",
        "        The parameter regularisation constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores theta_ after every gradient descent iteration\n",
        "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha = 0.01, tol=1e-4, max_iter = 100, theta_init = None, penalty = None, lambd = 0):\n",
        "        \n",
        "        # store meta-data\n",
        "        self.alpha = alpha\n",
        "        self.theta_init = theta_init\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "\n",
        "        self.theta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_theta_ = None\n",
        "    \n",
        "    def compute_cost(self, theta, X, y):\n",
        "    \n",
        "        \"\"\"\n",
        "        Compute the cost/objective function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        ...\n",
        "        # TODO ENDS\n",
        "\n",
        "    def compute_gradient(self, theta, X, y):\n",
        "    \n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        ...\n",
        "        # TODO ENDS\n",
        "\n",
        "    def has_converged(self, theta_old, theta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        theta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the target variable values for the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted target variables values for the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END"
      ],
      "metadata": {
        "id": "x_iD4A-TmjKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lin_reg_compute_cost(StudentLinearRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.881828654157736\n",
        "    \n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.94300429515773\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.919253244675344\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_compute_cost(LinearRegression)"
      ],
      "metadata": {
        "id": "0gOdODTWQQX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_compute_cost', answer = LinearRegression)"
      ],
      "metadata": {
        "id": "ct-hUcbC9Zp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lin_reg_compute_gradient(StudentLinearRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.53908485]\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.63908485]\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.66143613]\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_compute_gradient(LinearRegression)"
      ],
      "metadata": {
        "id": "pysdW3awRLl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_compute_gradient', answer = LinearRegression)"
      ],
      "metadata": {
        "id": "VptfbtsMAEVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lin_reg_has_converged(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
        "    student_ans = student_lr_reg.has_converged(test_case_theta_old, test_case_theta_new)\n",
        "    required_ans = True\n",
        "    \n",
        "    assert student_ans == required_ans\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_has_converged(LinearRegression)"
      ],
      "metadata": {
        "id": "naEgZPDXQ5_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_has_converged', answer = LinearRegression)"
      ],
      "metadata": {
        "id": "7rx213_3_gmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lin_reg_fit(StudentLinearRegression):\n",
        "    \n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.hist_theta_\n",
        "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
        "       [ 0.012     ,  0.00566085, -0.00773638],\n",
        "       [ 0.02351422,  0.01085581, -0.01491529],\n",
        "       [ 0.03457102,  0.01561393, -0.0215702 ],\n",
        "       [ 0.04519706,  0.01996249, -0.02773259],\n",
        "       [ 0.05541739,  0.02392713, -0.03343205]])\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_fit(LinearRegression)"
      ],
      "metadata": {
        "id": "NF9jGYOVSrC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_fit', answer = LinearRegression)"
      ],
      "metadata": {
        "id": "sewbA8dC5Fdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lin_reg_predict(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.predict(test_case_X)\n",
        "    required_ans = np.array([0.04739416, 0.02735934, 0.02140787, 0.04634383, 0.04320043,\n",
        "       0.02836861, 0.03726417, 0.03808224, 0.03214353, 0.05166998,\n",
        "       0.05102933, 0.05639199, 0.0416892 , 0.03175554, 0.04895695,\n",
        "       0.03465034, 0.02912364, 0.03954521, 0.0396391 , 0.06440433,\n",
        "       0.03189335, 0.06016748, 0.03661307, 0.07146111, 0.05261461,\n",
        "       0.04180017, 0.03223834, 0.0500466 , 0.06128615, 0.05703506,\n",
        "       0.05467262, 0.04388664, 0.04648138, 0.07052753, 0.04140456,\n",
        "       0.02830984, 0.05608863, 0.0212115 , 0.05238969, 0.05514024,\n",
        "       0.04020117, 0.05048966, 0.04696158, 0.04438422, 0.05897309,\n",
        "       0.05443805, 0.03375689, 0.04794345, 0.04242038, 0.04869202])\n",
        "    \n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_predict(LinearRegression)"
      ],
      "metadata": {
        "id": "uQQN2ky4UYxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_predict', answer = LinearRegression)"
      ],
      "metadata": {
        "id": "WKhfPR6uUJm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Synthetic dataset [Ungraded]**\n",
        "\n",
        "In this section we will first create some synthetic data on which we will run your linear regression implementation. We are creating 100 datapoints around the function y = mx + b, introducing Gaussian noise."
      ],
      "metadata": {
        "id": "hGlB3lojik_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't modify this cell\n",
        "if NOTEBOOK:\n",
        "    num_samples = 100\n",
        "\n",
        "    np.random.seed(1)\n",
        "    noise = np.random.randn(num_samples, 1)\n",
        "    X = np.random.randn(num_samples, 1)\n",
        "\n",
        "    y_ideal = 11*X + 5\n",
        "    y_real = (11*X + 5) + noise\n",
        "\n",
        "    plt.plot(X, y_real, 'ro')\n",
        "    plt.plot(X, y_ideal, 'b')"
      ],
      "metadata": {
        "id": "Rqp2jLGTiJQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that this data is clearly regressable with a line, which, ideally, would be 11x + 5\n",
        "\n",
        "Train a linear regression model using gradient descent, you should see that training loss goes down with the number of iterations and obtain a theta that converges to a value very close to [b, m], which in this case, for 11x + 5, would be theta = [5, 11]\n",
        "\n",
        "Also, notice the effect of the type of regularisation on the theta obtained (after convergence) as well as the testing MSE loss. Do they make sense, given what was discussed in class? "
      ],
      "metadata": {
        "id": "2G9qQh_uiXQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def test_synthetic_data_sgd(X, y, n_iter = 2000, penalty=None, lambd=0):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=37)\n",
        "  # Given that we want to get theta as the weights of the linear equation, we won't \n",
        "  # standardize in this section\n",
        "\n",
        "  alpha = 0.03  # Learning Rate\n",
        "\n",
        "  # # Train the model\n",
        "  lr_model = LinearRegression(alpha = alpha, tol=1e-4, max_iter = n_iter, penalty=penalty, lambd=lambd)\n",
        "  lr_model.fit(X_train,y_train[:, 0])\n",
        "  y_predict = lr_model.predict(X_test)\n",
        "  loss = sklearn.metrics.mean_squared_error(y_predict, y_test)\n",
        "  print(\" Theta: {} \\n Norm of Theta: {} \\n Testing MSELoss: {}\".format(lr_model.theta_, np.linalg.norm(lr_model.theta_, ord=2), loss))\n",
        "\n",
        "  loss_history = lr_model.hist_cost_\n",
        "  plt.plot(range(len(loss_history)), loss_history)\n",
        "  plt.title(\"OLS Training Loss\")\n",
        "  plt.xlabel(\"iteration\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  if penalty == \"l1\":\n",
        "    plt.title(\"L1 Regularised Training Loss\")\n",
        "  elif penalty == \"l2\":\n",
        "    plt.title(\"L2 Regularised Training Loss\")\n",
        "  plt.show()\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_synthetic_data_sgd(X, y_ideal, 500)\n",
        "    test_synthetic_data_sgd(X, y_ideal, 500, \"l1\", 0.02)\n",
        "    test_synthetic_data_sgd(X, y_ideal, 500, \"l2\", 0.02)"
      ],
      "metadata": {
        "id": "MYzOlitsiNCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3. Effect of polynomial degree on training and validation error [5 pts, manually graded] [optional for CIS 419]**\n",
        "\n",
        "Now, we consider a dataset that was generated using some higher degree polynomial function of the input variable. We do not know the degree of the underlying polynomial. Let us assume it to be an unknown value \"p\" and try to estimate it.\n",
        "\n",
        "Polynomial regression hypothesis for one input variable  or feature (x) can be written as:\n",
        "> $y = w_0 + w_1x + w_2x^2 + ... + w_px^p $\n",
        "\n",
        "If you observe carefully, this can still be solved as a linear regression, where, instead of just 2 weights, we have p+1 weights, and the new features are higher order terms of the original feature. Using this idea, in this section, we will investigate how changing the assumed polynomial degree \"p\" in our model affects the training and validation error."
      ],
      "metadata": {
        "id": "8qUcFLi6tjL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if NOTEBOOK:\n",
        "    \n",
        "    if not os.path.exists(\"cis519_hw2_poly_reg.csv\"):\n",
        "        !gdown --id 1F5cGpc9ayxf49RQskiJFlL0v5LzCP7dn\n",
        "    \n",
        "    poly_reg_df = pd.read_csv('cis519_hw2_poly_reg.csv')  "
      ],
      "metadata": {
        "id": "UcpXD4pTarFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression as LinearRegressionSklearn\n",
        "\n",
        "def polynomial_regression(poly_reg_df, degrees):\n",
        "    \"\"\"\n",
        "    Runs polynomial regression on the dataset 'poly_reg_df' for all the powers in 'degrees'\n",
        "    \"\"\"\n",
        "\n",
        "    loss_train_list = []\n",
        "    loss_test_list = []\n",
        "\n",
        "    X_base = poly_reg_df.iloc[:, :-1].values\n",
        "    y = poly_reg_df.iloc[:, -1].values\n",
        "\n",
        "    for d in degrees:\n",
        "\n",
        "        # TODO START: Complete the function:\n",
        "        # 1. Transform the base feature X_base into its polynomial features of degree 'd' using PolynomialFeatures\n",
        "        # Set include_bias to be False\n",
        "        X = ...\n",
        "        \n",
        "        # 2. Preprocessing and splitting into train/test (70-30 ratio and random_state as 42)\n",
        "        X_train, X_test, y_train, y_test = ...\n",
        "        \n",
        "        # 3. Scale X_train and X_test appropriately \n",
        "        ...\n",
        "        \n",
        "        # 4. Use scikit-learn's LinearRegression (imported as LinearRegressionSklearn for you) to \n",
        "        # fit a linear model between the scaled version of X_train and y_train\n",
        "        lr_model = LinearRegressionSklearn()\n",
        "        ...\n",
        "        \n",
        "        # 5. Obtain predictions of the model on train and test data\n",
        "        ...\n",
        "        \n",
        "        # 6. Compute the mean squared error and store it in loss_train and loss_test\n",
        "        loss_train = ...\n",
        "        loss_test = ...\n",
        "\n",
        "        # 7. Append loss_train to loss_train_list and loss_test to loss_test_list\n",
        "        ...\n",
        "\n",
        "    return loss_train_list, loss_test_list\n",
        "    # TODO END"
      ],
      "metadata": {
        "id": "qJMhCZ6SuepA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if NOTEBOOK:\n",
        "\n",
        "    degrees = np.arange(1, 9)\n",
        "    \n",
        "    loss_train_list, loss_test_list = polynomial_regression(poly_reg_df, degrees)\n",
        "\n",
        "    # TODO START:\n",
        "    # Plot the polynomial degrees (x-axis) against loss_train_list (y-axis) and loss_test_list (y-axis) in a single plot, with different colors.\n",
        "    # Make sure to include x and y axis labels, legend as well as the title\n",
        "    ...\n",
        "    # TODO END"
      ],
      "metadata": {
        "id": "aDiIc_XjuBqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attach the plot to your written homework solutions. Describe the trends in the plot obtained. Briefly explain the reasoning behind why this would happen.**"
      ],
      "metadata": {
        "id": "L-ByMGwo1TBS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78pB1H8vQ7qd"
      },
      "source": [
        "# **2. Logistic Regression** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph7a-NIkNyQ9"
      },
      "source": [
        "## **2.1. Logistic Regression Implementation [18 pts, autograded]**\n",
        "\n",
        "Implement logistic regression with both L1 and L2 regularization by completing the LogisticRegression class.  \n",
        "\n",
        "Your class must implement the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
        "* `sigmoid(x)`\n",
        "* `compute_cost(theta, X, y)`\n",
        "* `compute_gradient(theta, X, y)`\n",
        "* `has_converged(theta_old, theta_new)`\n",
        "* `fit(X, y)`\n",
        "* `predict_proba(X)`\n",
        "* `predict(X)`\n",
        "\n",
        "Note that these methods have already been defined correctly for you in the LogisticRegression class. **DO NOT** change the API.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.1. Sigmoid Function [1 pt]**\n",
        "\n",
        "You should begin by implementing the `sigmoid` function.  As you may know, the sigmoid function $\\sigma(x)$ is mathematically defined as follows.\n",
        "\n",
        "> $\\sigma(x) = \\frac{1}{1\\ +\\ \\text{exp}(-x)}$\n",
        "\n",
        "**Be certain that your sigmoid function works with both vectors and matrices** --- for either a vector or a matrix, you function should perform the sigmoid function on every element.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.2. Cost Function [5 pts]**\n",
        "\n",
        "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
        "\n",
        "> $\n",
        "\\mathcal{L}({\\theta}) = -\\sum_{i =1}^N [ y_i\\log(h_{{\\theta}}({x}_i)) + (1 - y_i)\\log(1 - h_{{\\theta}}({x}_i))]\n",
        "$\n",
        "\n",
        "where\n",
        "> $\n",
        "h_{\\theta}(x_{i}) = \\sigma(\\theta^{T}x_{i})\n",
        "$\n",
        "\n",
        "\n",
        "L1 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  |{\\theta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  {\\theta}_j^2 \n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.3. Gradient of the Cost Function [5 pts]**\n",
        "\n",
        "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.4. Convergence Check [1 pt]**\n",
        "\n",
        "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer 2.1.5 for convergence condition.\n",
        " \n",
        "---\n",
        "\n",
        "### **2.1.5. Training [3 pts]**\n",
        "\n",
        "The `fit` method should train the model via gradient descent, relying on the cost and gradient functions. The trained weights/coefficients must be stored as `theta_`. The weights start as a zero vector. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). \n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the gradient descent algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.6. Predict Probability [1 pt]**\n",
        "\n",
        "The `predict_probability` function should predict the probabilities that the data points in a given input data matrix belong to class 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.7. Predict [2 pts]**\n",
        "\n",
        "The `predict` function should predict the classes of the data points in a given input data matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeWSs7YNQ9ue"
      },
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol : float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty : string, default = None\n",
        "        The type of regularization. The other acceptable options are l1 and l2\n",
        "    lambd : float, default = 1.0\n",
        "        The parameter regularisation constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores theta_ after every gradient descent iteration\n",
        "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.01, tol=0.0001, max_iter=10000, theta_init=None, penalty = None, lambd = 1.0):\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.theta_init = theta_init\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "\n",
        "        self.theta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_theta_ = None\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the sigmoid value of the argument.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: numpy.ndarray\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out: numpy.ndarray\n",
        "            The sigmoid value of x\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def compute_cost(self, theta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the cost/objective function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        # DO NOT use np.dot for this function as it can possibly return nan. Use a combination of np.nansum and np.multiply.\n",
        "        ...\n",
        "        # TODO END\n",
        "        \n",
        "    def compute_gradient(self, theta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def has_converged(self, theta_old, theta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        theta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the probabilities that the data points in X belong to class 1.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted probabilities that the data points in X belong to class 1\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the classes of the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy.ndarray of shape (N,)\n",
        "            The predicted class of the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-VyGzobU4d3"
      },
      "source": [
        "def test_log_reg_sigmoid(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    test_case = np.array([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n",
        "    student_ans = student_lr_clf.sigmoid(test_case)\n",
        "    required_ans = np.array([0.83539354, 0.35165864, 0.3709434 , 0.25483894, 0.70378922])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_sigmoid(LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvem30ziTrrs"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_sigmoid', answer = LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR5a5eEsVbpj"
      },
      "source": [
        "def test_log_reg_compute_cost(StudentLogisticRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.467975765663204\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.52915138076548\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.505400330283089\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_compute_cost(LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAWvDLS0_Ejb"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_compute_cost', answer = LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP5ehbmv6aQa"
      },
      "source": [
        "def test_log_reg_compute_gradient(StudentLogisticRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.20203139])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.30203139])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.32438267])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_compute_gradient(LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oehKOwNn_Ion"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_compute_gradient', answer = LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5Jx2O4M79sd"
      },
      "source": [
        "def test_log_reg_has_converged(StudentLogisticRegression):\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
        "    student_ans = student_lr_clf.has_converged(test_case_theta_old, test_case_theta_new)\n",
        "    required_ans = True\n",
        "\n",
        "    assert student_ans == required_ans\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_has_converged(LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX_D-6Ag_K76"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_has_converged', answer = LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZf2uH61FbYM"
      },
      "source": [
        "def test_log_reg_fit(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.hist_theta_\n",
        "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
        "                             [ 0.005     , -0.00597503,  0.00564325],\n",
        "                             [ 0.01006813, -0.01184464,  0.0111865 ],\n",
        "                             [ 0.01520121, -0.01761226,  0.01663348],\n",
        "                             [ 0.02039621, -0.02328121,  0.02198778],\n",
        "                             [ 0.02565018, -0.0288547 ,  0.02725288]])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_fit(LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3geNT88Ivqm"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_fit', answer = LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKywIL8gIyw0"
      },
      "source": [
        "def test_log_reg_predict_proba(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.predict_proba(test_case_X)\n",
        "    required_ans = np.array([0.49052814, 0.5029122 , 0.48449386, 0.48864172, 0.50241207])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_predict_proba(LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUzQe-7BLYAX"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_predict_proba', answer = LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_log_reg_predict(StudentLogisticRegression):\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.predict(test_case_X)\n",
        "    required_ans = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
        "                             0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])\n",
        "\n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 0.02\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_predict(LogisticRegression)"
      ],
      "metadata": {
        "id": "nuvyk1SbNv_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBZk4fwJOdI2"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_predict', answer = LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2. Effect of learning rate on gradient descent [5 pts, manually graded]**\n",
        "\n"
      ],
      "metadata": {
        "id": "zAVp1ZEkL2l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to download the dataset."
      ],
      "metadata": {
        "id": "qxNxtdHnRLws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if NOTEBOOK:\n",
        "    \n",
        "    if not os.path.exists(\"cis519_hw2_admit.csv\"):\n",
        "        !gdown --id 1CSD1vA9qZucuevxCuaOwr91tBaZcjNNh\n",
        "    \n",
        "    train_df = pd.read_csv(\"cis519_hw2_admit.csv\")"
      ],
      "metadata": {
        "id": "c6W4LVZgMl3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains two features - scores in two exams and the target variable is whether the student was admitted into a college or not. Your task for this question is to use this dataset and plot the variation of cost function with respect to the number of gradient descent iterations for different learning rates. Perform the following steps.\n",
        "\n",
        "1. Scale the features using StandardScaler\n",
        "2. For each of the learning rates - {0.001, 0.01, 0.1, 0.25}, fit a logistic regression model to the scaled data by running a maximum of 100 iterations of gradient descent with L2 penalty and $\\lambda$ as 0.001.\n",
        "3. Show the variation of the cost (stored in `hist_cost_`) with respect to the number of iterations for all the learning rates in the same plot.\n",
        "\n",
        "Submit the plot along with the written homework solutions. The plot should have an appropriate title, axes labels, and legend. Briefly comment on the effect of increasing learning rate and what would be the best learning rate among the four values based on the plot."
      ],
      "metadata": {
        "id": "mqzaeWj1PIa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if NOTEBOOK:\n",
        "    # STUDENT CODE STARTS:\n",
        "    ...\n",
        "    # STUDENT CODE ENDS"
      ],
      "metadata": {
        "id": "tR1ZrrxYM36e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the .ipynb notebook and submit on Gradescope."
      ],
      "metadata": {
        "id": "YYRykDPr9U8A"
      }
    }
  ]
}