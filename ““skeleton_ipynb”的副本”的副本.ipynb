{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "““skeleton.ipynb”的副本”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeyuJasonNiu/CIS519/blob/main/%E2%80%9C%E2%80%9Cskeleton_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnokyRPqBJ7n"
      },
      "source": [
        "# **CIS 419/519 Homework 1 - Spring 2022**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kqEqQ8-ezg3"
      },
      "source": [
        "**Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. This is the master notebook so <u>you will not be able to save your changes without copying it </u>! Once you click on that, make sure you are working on that version of the notebook so that your work is saved** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXCMO-KSHept"
      },
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpaeiBrHz7Xy"
      },
      "source": [
        "# For autogreader only, do not modify this cell. \n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJsS5fTThkLU"
      },
      "source": [
        "## **PennGrader Setup**\n",
        "First, you'll need to set up the PennGrader, an autograder we are going to use throughout the semester. The PennGrader will automatically grade your answer and provide you with an instant feedback. Unless otherwise stated, you can resubmit up to a reasonable number of attempts (e.g. 100 attemptes per day). **We will only record your latest score in our backend database**. \n",
        "\n",
        "After finishing each homework assignment, you must submit your iPython notebook to gradescope before the homework deadline. Gradescope will then retrive and display your scores from our backend database. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF4jSeEqhkLV"
      },
      "source": [
        "%%capture\n",
        "!pip3 install penngrader --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DolsQ5fDhkLV"
      },
      "source": [
        "from penngrader.grader import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppXdHvskhkLV"
      },
      "source": [
        "## **Autograder Setup**\n",
        "Enter your 8-digit PennID below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7nuuZCohkLW"
      },
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO \n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = ...          # YOUR PENN-ID GOES HERE AS AN INTEGER#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuZcxcXJhkLW"
      },
      "source": [
        "Run the following cell to initialize the autograder. This autograder will let you submit your code directly from this notebook and immediately get a score.\n",
        "\n",
        "**NOTE:** Remember we store your submissions and check against other student's submissions... so, not that you would, but no cheating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omOwgdzohkLW"
      },
      "source": [
        "grader = PennGrader(homework_id = 'CIS_519_Spring22_HW1', student_id = STUDENT_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E7O8ygZvL-H"
      },
      "source": [
        "## **Datasets**\n",
        "Next, we will download all datasets from Google Drive to your local runtime. After successful download, you may verify that all datasets are present in your colab instance.\n",
        "\n",
        "HW1 Datasets:\n",
        "- [cis519_hw1_diabetes_train.csv](https://drive.google.com/file/d/1wfZ375m-HOvtWb8fBUjZ2zbW5nkXtlVx/view?usp=sharing)\n",
        "- [cis519_hw1_diabetes_test.csv](https://drive.google.com/file/d/14DGsr_eIHRGnDw4o7FVBFUVfg_MljzzO/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5THxiIJBvH_F"
      },
      "source": [
        "if NOTEBOOK:\n",
        "    if not os.path.exists(\"cis519_hw1_diabetes_train.csv\"):\n",
        "        !gdown --id 1wfZ375m-HOvtWb8fBUjZ2zbW5nkXtlVx\n",
        "    if not os.path.exists(\"cis519_hw1_diabetes_X_test.csv\"):\n",
        "        !gdown --id 14DGsr_eIHRGnDw4o7FVBFUVfg_MljzzO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **NOTE: Results of sections marked as \"manually graded\" should be submitted along with the written homework solutions.**"
      ],
      "metadata": {
        "id": "3RUGn-V6Nnjt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWRRLni5Pz_Q"
      },
      "source": [
        "# **1. K-Nearest Neighbors [10 pts]**\n",
        "While doing classification, KNN searches the memorized training instances for the K instances that most closely resemble the new instance and assigns to it the most common class. An alternate way of understanding KNN is by looking at the learned decision boundaries. In this problem, you will implement a function to classify points in the X-Y coordinates using [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). The training dataset used is the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris), and each point in the 2d-space will be classified into one of the three classes using its x-coordinate(sepal length) and y-coordinate(sepal width)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1. Load Iris Dataset**\n",
        "Please complete the load_dataset function to\n",
        "- Populate X_train with iris dataset features. We use only the sepal length and width for this exercise, i.e. the first two columns in the dataset\n",
        "- Populate y_train with labels (species)\n",
        "- return X_train and y_train"
      ],
      "metadata": {
        "id": "mze8zKRY4c3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "def load_iris_dataset():\n",
        "    '''\n",
        "    Args:\n",
        "        None\n",
        "    Returns: X_train, y_train\n",
        "    Notes:\n",
        "        1. Please do not change the provided code\n",
        "    '''\n",
        "    # import training data\n",
        "    iris = datasets.load_iris()\n",
        "\n",
        "    # TODO:\n",
        "    # Examine the iris variable and initialize the following variables appropriately:\n",
        "    # 1. X_train - Shape (m, 2): Only use the sepal length and width\n",
        "    # 2. y_train - Shape (m, ): target labels \n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "# Load the iris dataset first\n",
        "X_train, y_train = load_iris_dataset()"
      ],
      "metadata": {
        "id": "ay4oEx1x4eeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Standardise the Features [4 pts, autograded]**\n",
        "Please complete the standardise_features function to \n",
        "standardize the features by subtracting the mean and scaling to unit variance. i.e \n",
        "\n",
        " z = (x - u) / s\n",
        "\n",
        "where u is the mean of the training data and s is the standard deviation of the training data.\n",
        "\n",
        "Here, centering and scaling need to happen independently on each feature (column) of the training data.\n",
        "\n",
        "**Note**: \n",
        "\n",
        "Please implement this function yourself. \n",
        "\n",
        "**Do NOT use sklearn's StandardScaler**. \n",
        "\n",
        "You are encouraged to use numpy as well as numpy vectorisation/broadcasting techniques to speed up the calculations."
      ],
      "metadata": {
        "id": "RYAL32DU2cWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardise_features(X_train):\n",
        "\n",
        "  '''\n",
        "  Args:\n",
        "      X_train: Training dataset\n",
        "  Returns: X_train (After Standardization)\n",
        "  Notes:\n",
        "      1. Please do not change the provided code\n",
        "  '''\n",
        "\n",
        "  # TODO:\n",
        "  # 1. Calculate columnwise means and standard deviations\n",
        "  # 2. Perform columnwise standardisation i.e. subtract off mean and divide by standard deviation\n",
        "  # 3. Return the standardised data\n",
        "  #### Student code starts ####\n",
        "  ...\n",
        "  #### Student code ends ####\n",
        "\n",
        "X_train = standardise_features(X_train)"
      ],
      "metadata": {
        "id": "yn2eDuOj170T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_knn_standardise', answer = standardise_features)"
      ],
      "metadata": {
        "id": "8Z2ZFHXN2EVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHobVEPHxXa8"
      },
      "source": [
        "## **1.3. Plot KNN Decision Boundary [6 pts, manually graded]**\n",
        "Please complete the plot_KNN_boundary function to\n",
        "- train a KNN classifier with k neighbors using the provided X_train and y_train\n",
        "- make predictions using X_test and save the result as 'y_test'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def plot_KNN_boundary(k, X_train, y_train):\n",
        "    '''\n",
        "    Args:\n",
        "        k: Number of neighbors to use for kneighbors queries.\n",
        "        X_train: Training dataset\n",
        "        y_train: Labels\n",
        "\n",
        "    Returns:\n",
        "    Notes:\n",
        "        1. Please do not change the provided code\n",
        "        2. save the predicted labels as y_test for plotting\n",
        "    '''\n",
        "\n",
        "    # Mesh 2d space into grid to generate X_test and y_test\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                            np.arange(y_min, y_max, h))\n",
        "    X_test = np.c_[xx.ravel(), yy.ravel()]\n",
        "    y_test = np.zeros(xx.shape)\n",
        "\n",
        "    # TODO:\n",
        "    # 1. train a KNN classifier\n",
        "    # 2. save the predictions on X_test in y_test\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    y_test = y_test.reshape(xx.shape)\n",
        "    cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
        "    cmap_bold = ['darkorange', 'c', 'darkblue']\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, y_test, cmap=cmap_light)\n",
        "\n",
        "    # Also plot the training points\n",
        "    iris_target_names = ['setosa', 'versicolor', 'virginica']\n",
        "    sns.scatterplot(x=X_train[:, 0], y=X_train[:, 1], hue=map(lambda y: iris_target_names[y], y_train),\n",
        "                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.title(\"3-Class classification (k = %i)\" % (k))\n",
        "    plt.xlabel(\"standardised sepal length\")\n",
        "    plt.ylabel(\"standardised sepal width\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KQNVfjcz4tj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTvb0XeDyAEC"
      },
      "source": [
        "Explore the effect of changing k on the learned decision boundaries. \n",
        "- Submit the plots along with the written homework solutions. \n",
        "- State whether the model underfits/overfits as k increases and explain why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbUJFxTcqTgP"
      },
      "source": [
        "# Plot KNN decision boundaries with different k values\n",
        "def visualize_KNN(X_train, y_train):\n",
        "\n",
        "    k_list = [1, 4, 11, 15]\n",
        "    \n",
        "    # TODO:\n",
        "    # 1. Call plot_KNN_boundary function for each value of k in k_list\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "if NOTEBOOK:\n",
        "    visualize_KNN(X_train, y_train) ### Comment out this line when submitting ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Cm728599Wt"
      },
      "source": [
        "# **2. Measures of Impurity and their Reduction [15 pts]**\n",
        "To grow a classification tree, instead of a binary error (1/0), measures of impurity are used to see how good a leaf node is. Recall that we discussed about entropy being one such measure of impurity. We will be working with entropy and comparing it to another metric called the gini index. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FHSKW6f_7yg"
      },
      "source": [
        "## **2.1. Measures of Impurity [9 pts]**\n",
        "\n",
        "For this problem, consider that you have a binary classification problem of two classes, the positive class $1$ and the negative class $0$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aELrA7A_WbJ8"
      },
      "source": [
        "### **2.1.1. Entropy [2 pts, autograded]**\n",
        "\n",
        "Please complete the entropy function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dyEposN3Sph"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cross_entropy(prob_class1):  \n",
        "\n",
        "    \"\"\"\n",
        "    Returns the cross-entropy value of a node given the probability of a sample belonging to class 1 in the node.\n",
        "\n",
        "    Args: \n",
        "        prob_class1: The probability of a sample belonging to class 1 in a decision tree node\n",
        "      \n",
        "    Returns:\n",
        "        ce: The cross-entropy value for the node\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends #### \n",
        "    \n",
        "assert cross_entropy(0.5) == 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE5spiSJp8e7"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_cross_entropy', answer = cross_entropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.2. Gini Index [2 pts, autograded]**\n",
        "\n",
        "Gini index is another measure of impurity. For an K-class classification problem, gini index is calculated as follows.\n",
        "\n",
        "$$\\text{Gini Index} = \\sum_{k = 1}^{K} p_k(1 - p_k)$$\n",
        "\n",
        "Complete the following function for calculating the gini index of a binary-class problem (k = 2)."
      ],
      "metadata": {
        "id": "kxp71lMkdoVP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj__P_XV3Uj5"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def gini_index(prob_class1):  \n",
        "\n",
        "    \"\"\"\n",
        "    Returns the gini-index value of a node given the probability of a sample belonging to class 1 in the node.\n",
        "\n",
        "    Args: \n",
        "        prob_class1: The probability of a sample belonging to class 1 in a decision tree node\n",
        "      \n",
        "    Returns:\n",
        "        gi: The gini-index value for the node\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends #### \n",
        "\n",
        "assert gini_index(0.5) == 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_gini_index', answer = gini_index)"
      ],
      "metadata": {
        "id": "yMOROQZH09Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "837Ex93tXP1c"
      },
      "source": [
        "### **2.1.2. Plot [5 pts, manually graded]**\n",
        "\n",
        "Please complete the impurity_measures_plot function and generate a plot of the entropy and gini index values with respect to the class 1 probability values. Both the impurity measures should be on the same plot.\n",
        "\n",
        "- Submit the generated plot along with the written homework solutions.\n",
        "- Make sure the plot has a title, legend and axes labels.\n",
        "- Comment on why cross entropy and gini index are suitable measures of impurity based on the plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1lZ_hdi-Bwc"
      },
      "source": [
        "def impurity_measures_plot():\n",
        "\n",
        "    '''\n",
        "    Plots the cross entropy and gini index values with respect to the probability values of class 1.\n",
        "\n",
        "    Args: \n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Notes:\n",
        "        1. Please do not change the provided code\n",
        "        2. Both cross entropy and gini index should be on the same scatter plot\n",
        "    '''\n",
        "\n",
        "    prob_class1_arr = np.arange(1, 1000)/1000\n",
        "    ce_arr = np.array([cross_entropy(p) for p in prob_class1_arr])\n",
        "    gi_arr = np.array([gini_index(p) for p in prob_class1_arr])\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "if NOTEBOOK:\n",
        "  impurity_measures_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow8qK0lZ2_JM"
      },
      "source": [
        "## **2.2. Reduction in Impurity [6 pts]**\n",
        "\n",
        "Recall that we also discussed information gain which is the change in entropy from the parent node to the children nodes. Gini reduction is similar to information gain except you replace entropy values with gini index."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.1. Information Gain [3 pts, autograded]**"
      ],
      "metadata": {
        "id": "pMeFW6DgPCFe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZvwxSjc3dHT"
      },
      "source": [
        "def information_gain(num_samples_parent, num_class1_parent, num_samples_child1, num_class1_child1):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Args: \n",
        "        num_samples_parent: Number of samples in the parent node\n",
        "        num_class1_parent: Number of samples of class 1 in parent node\n",
        "        num_samples_child1: Number of samples in the first child node\n",
        "        num_class1_child1: Number of samples of class 1 in the first child node\n",
        "\n",
        "    Returns:\n",
        "        ig: Information Gain\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    # 1. You will need to calculate cross-entropy for the parent and child nodes\n",
        "    # 2. Use the above entropies to finally calculate information gain\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "\n",
        "assert np.abs(information_gain(100, 60, 30, 5) - 0.251) < 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GELCUsbFG18O"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_information_gain', answer = information_gain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.2. Gini Reduction [3 pts, autograded]**"
      ],
      "metadata": {
        "id": "5GUxtCnOPTBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_reduction(num_samples_parent, num_class1_parent, num_samples_child1, num_class1_child1):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Args: \n",
        "        num_samples_parent: Number of samples in the parent node\n",
        "        num_class1_parent: Number of samples of class 1 in parent node\n",
        "        num_samples_child1: Number of samples in the first child node\n",
        "        num_class1_child1: Number of samples of class 1 in the first child node\n",
        "\n",
        "    Returns:\n",
        "        gr: Gini Reduction\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "assert np.abs(gini_reduction(100, 60, 30, 5) - 0.161) < 0.01"
      ],
      "metadata": {
        "id": "bCiXGB--6MR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_gini_reduction', answer = gini_reduction)"
      ],
      "metadata": {
        "id": "ldMLkWpS-cLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JG4XjHNPuC2"
      },
      "source": [
        "# **3. Decision Tree [35 pts]**\n",
        "\n",
        "In this section you will be training a decision tree classifier to predict the presence of diabetes in a person given various input features. The diabetes dataset that we are using is from the [2013-2014  National Health and Nutrition Examination Survey (NHANES)](https://wwwn.cdc.gov/nchs/nhanes/Default.aspx). We have reduced the dataset to only 20 features but the original dataset had over 1,800 features. `cis519_hw1_diabetes_train.csv` and `cis519_hw1_diabetes_X_test.csv` are the datasets that you would be using for training and testing respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1. Load Datasets**\n",
        "\n",
        "Read the files `cis519_hw1_diabetes_train.csv` and `cis519_hw1_diabetes_X_test.csv` into train_df and test_df respectively in the `load_diabetes_datasets` function."
      ],
      "metadata": {
        "id": "KC_OXMz-oRKD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ4pBCjm_5-K"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_diabetes_datasets():\n",
        "    '''\n",
        "    Args:\n",
        "        None\n",
        "    Returns: \n",
        "        train_df, test_df\n",
        "    '''\n",
        "    \n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2. Preprocess Datasets [10 pts, autograded]**\n",
        "\n",
        "The datasets we have provided are not ready-to-use for machine learning and requires preprocessing. We want you to perform feature selection and handle missing values in both the training and test datasets. \n",
        "\n",
        "### **3.2.1. Feature Selection**\n",
        "\n",
        "For feature selection, you should retain the following features at least and experiment including/excluding the remaining features. \n",
        "\n",
        "- 'RIDAGEYR'\n",
        "- 'BMXWAIST'\n",
        "- 'BMXHT'\n",
        "- 'LBXTC'\n",
        "- 'BMXLEG'\n",
        "- 'BMXWT'\n",
        "- 'BMXBMI'\n",
        "- 'RIDRETH1'\n",
        "- 'BPQ020'\n",
        "- 'ALQ120Q'\n",
        "- 'DMDEDUC2'\n",
        "- 'RIAGENDR'\n",
        "- 'INDFMPIR'\n",
        "\n",
        "The column `DIABETIC` in the training dataset is the target variable. \n",
        "\n",
        "### **3.2.2. Handling Missing Values**\n",
        "\n",
        "We recommend you to drop rows with missing values in the training set. However, you should not drop rows with missing values in the test set. Instead, you should impute missing values in the test set with the mean of the corresponding columns in the training set."
      ],
      "metadata": {
        "id": "_06na9yloolX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoEdsqDgBC49"
      },
      "source": [
        "if NOTEBOOK:\n",
        "    train_df, test_df = load_diabetes_datasets()\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_datasets(train_df, test_df):\n",
        "    '''\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "    Returns:\n",
        "        train_df (preprocessed)\n",
        "        test_df (preprocessed)\n",
        "    Note:\n",
        "        1. At least the following columns should be present in the final train_df:\n",
        "            - 'RIDAGEYR'\n",
        "            - 'BMXWAIST'\n",
        "            - 'BMXHT'\n",
        "            - 'LBXTC'\n",
        "            - 'BMXLEG'\n",
        "            - 'BMXWT'\n",
        "            - 'BMXBMI'\n",
        "            - 'RIDRETH1'\n",
        "            - 'BPQ020'\n",
        "            - 'ALQ120Q'\n",
        "            - 'DMDEDUC2'\n",
        "            - 'RIAGENDR'\n",
        "            - 'INDFMPIR'\n",
        "            - 'DIABETIC'\n",
        "        2. test_df will have all the columns in train_df except the 'DIABETIC' column \n",
        "        3. Drop any rows in train_df that have missing values\n",
        "        4. DO NOT drop rows with missing values test_df. Impute missing values in test_df with the means of the corresponding columns in train_df. \n",
        "    '''\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if NOTEBOOK:\n",
        "    # PennGrader Grading Cell\n",
        "    grader.grade(test_case_id = 'test_preprocess_datasets', answer = preprocess_datasets(train_df, test_df))"
      ],
      "metadata": {
        "id": "nkhFIXIFZdcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3. Decision Tree Training with Pruning [14 pts autograded]**\n",
        "\n",
        "Next, we will be fitting a decision tree classifier and prune the tree appropriately. The `DecisionTreeClassifier` in scikit-learn uses a way of pruning called **Minimal Cost-Complexity Pruning**. We won't cover the specifics, but you can learn more from this [link](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning) if you wish. But, you don't need to learn the details in order to use it effectively. The amount of pruning is entirely dependent on the value of the `ccp_alpha` parameter. In order to tune the `ccp_alpha` parameter, you will use [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html). The purpose of cross-validation is to estimate how well a model will generalize on unseen data.\n",
        "\n",
        "Implement the function `best_ccp_alpha_f1` to do automatic tuning of the `ccp_alpha` parameter.  Your function should vary the value of the `ccp_alpha` parameter and return the value for `ccp_alpha` with the highest cross-validation F1 score over the given dataset `train_df`. The sklearn library has a [built-in function](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path) to generate a list of effective ccp_alphas. Given the imbalanced nature of the dataset, most of the people in the data set are non-diabetic. You can get a model with very high test accuracy by always predicting no one is diabetic. To address this problem, more importance should be given to the [F1 score](https://en.wikipedia.org/wiki/F-score) of your model rather than the classification accuracy.\n",
        "\n",
        "Once you are done with your modeling process, test your model on the test dataset and output your predictions into the PennGrader.\n",
        "\n",
        "For this problem, you need to have at least 80% accuracy and a F1 score of 0.2 on the test dataset to get full points."
      ],
      "metadata": {
        "id": "w0k07SoBrrD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "if NOTEBOOK:\n",
        "    train_df, test_df = preprocess_datasets(train_df, test_df)\n",
        "\n",
        "def best_ccp_alpha_f1(train_df):\n",
        "    \"\"\"\n",
        "    Returns the pruning parameter (best_ccp_alpha) with the highest cross-validation F1 score along with the \n",
        "    five cross-validation F1 scores corresponding (cv_f1_scores).\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "\n",
        "    Returns:\n",
        "        best_ccp_alpha: the tuned best ccp alpha value\n",
        "        cv_f1_scores: the five cross-validation F1 scores\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####    \n",
        "\n",
        "def refit_and_predict(train_df, test_df, best_ccp_alpha):\n",
        "    \"\"\"\n",
        "    Fit a decision tree classifier on the training data using the best_ccp_alpha value and output the predictions on the\n",
        "    test set.\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "        best_ccp_alpha\n",
        "\n",
        "    Returns:\n",
        "        y_test_pred: The predicted values for the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code starts ####\n",
        "\n",
        "if NOTEBOOK:\n",
        "    best_ccp_alpha, cv_f1_scores = best_ccp_alpha_f1(train_df)\n",
        "    y_test_pred = refit_and_predict(train_df, test_df, best_ccp_alpha)"
      ],
      "metadata": {
        "id": "HOlr7qzjxIxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if NOTEBOOK:\n",
        "    # PennGrader Grading Cell\n",
        "    grader.grade(test_case_id = 'test_refit_and_predict', answer = y_test_pred)"
      ],
      "metadata": {
        "id": "7K2RNN3Tg6jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khcu-0rBipeR"
      },
      "source": [
        "## **3.4. Computing Confidence Intervals [5 pts, autograded]**\n",
        "\n",
        "Even though you may have computed the average F1 score across the held-out folds during cross validation, how confident can you be that the number you computed is the true F1 score for that set of features? If you try rerunning your code with a different random seed, you may actually get a different F1 score. But which one is right?\n",
        "\n",
        "In order to answer this question, we will compute a confidence interval based on the Student's t-distribution, which will tell us with 99\\% confidence that the true mean is within a lower and upper bound. To compute the confidence interval, we need to compute the sample mean, $\\bar{x}$, sample standard deviation, $S$, and the number of observations for each classifier, $n$. ***In our specific case, the number of observations should be 5 because we have 5 reported F1 scores from cross-validation.***\n",
        "\n",
        "Then, the confidence interval is computed by\n",
        "    \n",
        "$$\\bar{x} \\pm t \\cdot \\frac{S}{\\sqrt{n}}$$\n",
        "\n",
        "Here, $t$ is the critical value, which we can look up using the provided t-table (https://www.stat.colostate.edu/inmem/gumina/st201/pdf/Utts-Heckard_t-Table.pdf). For example, when $n=10$, if we are looking for a 99\\% confidence interval, then the number in the 99\\% confidence column with degrees of freedom of $n-1=9$ would be $t=3.25$. Then, we can plug in all of the statistics into the confidence interval formula and get a range of values for which we are 99\\% confident that the true F1 score of the classifier falls between.\n",
        "\n",
        "For this computation, we should use the unbiased estimator of the variance, which means that the degrees of freedom on the standard deviation calculation must be set. Look in the optional arguments of np.std to learn more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mDdt6GfddPn"
      },
      "source": [
        "def calculate_confidence_interval(cv_f1_scores):\n",
        "    '''\n",
        "    Args:\n",
        "        cv_f1_scores      :   np.array, reported cross-validation F1 scores\n",
        "    Returns:\n",
        "        interval    :   np.array, lower bound and upper bound of the 99% confidence interval\n",
        "    '''\n",
        "    \n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code starts ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7oV-VY03B6S"
      },
      "source": [
        "def test_confidence_intervals():\n",
        "    data = np.array([15.6, 16.2, 22.5, 20.5, 16.4])\n",
        "    result = np.round(calculate_confidence_interval(data), 3)\n",
        "    interval = np.array([11.918, 24.562])\n",
        "    assert (np.array_equal(interval, result))\n",
        "\n",
        "test_confidence_intervals()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "586Wom54xlVw"
      },
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_confidence_intervals', answer = calculate_confidence_interval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.5. Performance Table [6 pts, manually graded]**\n",
        "\n",
        "Repeat the process for two other sets of features and present a performance table (like the one shown below) that compares the F1 scores and confidence intervals of the three sets of features, indicating which one is your chosen best set. Remember that each set should include the 13 features mentioned earlier. As mentioned earlier, submit this table along with the written homework solutions as this is manually graded.\n",
        "\n",
        "---\n",
        "\n",
        "S.No. | Features | Best CCP Alpha | Mean Cross-validation F1 Score | Cross-validation F1 Score Confidence Interval\n",
        "--- | --- | --- | --- | ---\n",
        "1 | Set 1 | | |\n",
        "2 | Set 2 | | |\n",
        "3 | Set 3 | | |\n"
      ],
      "metadata": {
        "id": "ajUkK26YLA1i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFPkrjyp5vub"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOT4XbEE6MKF"
      },
      "source": [
        "- Submit the notebook as a `.ipynb` file to the coding portion of the Gradescope submission. This can be done in Google Colab via the `File - Download .ipynb` menu option."
      ]
    }
  ]
}